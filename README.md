# Healthcare LLM Guardrails â€“ AI_NEMO

A lightweight healthcare AI safety framework built using NVIDIA NeMo Guardrails to ensure safe, compliant, and responsible conversational AI interactions.

---

## ğŸ“Œ Overview

This project implements guardrails for healthcare conversational AI systems to:

- Prevent unsafe medical advice
- Protect patient privacy
- Resist prompt injection attacks
- Detect crisis situations
- Provide safe health information with appropriate disclaimers

---

## ğŸ›  Tech Stack

- Python
- NVIDIA NeMo Guardrails
- Flask
- HTML / CSS

---

## ğŸš€ How to Run

1. Clone the repository:
   git clone https://github.com/ishwariwakchaure5/healthcare-llm-guardrails-AI_NEMO.git

2. Navigate to the project folder:
   cd healthcare-llm-guardrails-AI_NEMO

3. Install dependencies:
   pip install -r requirements.txt

4. Run the application:
   python app.py

---

## ğŸ“‚ Project Structure

healthcare-llm-guardrails-AI_NEMO/
â”‚
â”œâ”€â”€ app.py
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ rails/
â”œâ”€â”€ templates/
â”œâ”€â”€ static/
â””â”€â”€ README.md

---

## ğŸ‘©â€ğŸ’» Author

Ishwari Wakchaure  
GitHub: https://github.com/ishwariwakchaure5
